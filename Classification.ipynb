{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#LINEAR REGRESSION MODEL WITH GIVEN DATA \n",
    "\n",
    "#CLASS A (1)\n",
    "A=[[0,0,1,0,0],\n",
    "  [0,1,0,1,0],\n",
    "  [1,0,0,0,1],\n",
    "  [1,1,1,1,1],\n",
    "  [1,0,0,0,1]]\n",
    "B=[[1,1,1,0,0],\n",
    "  [1,0,0,1,0],\n",
    "  [1,1,1,0,0],\n",
    "  [1,0,0,1,0],\n",
    "  [1,1,1,0,0]]\n",
    "C=[[0,1,1,0,0],\n",
    "  [1,0,0,1,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,0,0,1,0],\n",
    "  [0,1,1,0,0]]\n",
    "D=[[1,1,1,0,0],\n",
    "  [1,0,0,1,0],\n",
    "  [1,0,0,1,0],\n",
    "  [1,0,0,1,0],\n",
    "  [1,1,1,0,0]]\n",
    "E=[[1,1,1,1,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,1,1,1,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,1,1,1,0]]\n",
    "\n",
    "#CLASS B (0)\n",
    "F=[[1,1,1,1,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,1,1,0,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,0,0,0,0]]\n",
    "G=[[1,1,1,1,0],\n",
    "  [1,0,0,1,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,0,1,1,1],\n",
    "  [1,1,1,1,0]]\n",
    "H=[[1,0,0,0,1],\n",
    "  [1,0,0,0,1],\n",
    "  [1,1,1,1,1],\n",
    "  [1,0,0,0,1],\n",
    "  [1,0,0,0,1]]\n",
    "I=[[1,1,1,1,1],\n",
    "  [0,0,1,0,0],\n",
    "  [0,0,1,0,0],\n",
    "  [0,0,1,0,0],\n",
    "  [1,1,1,1,1]]\n",
    "J=[[1,1,1,1,0],\n",
    "  [0,1,0,0,0],\n",
    "  [0,1,0,0,0],\n",
    "  [0,1,0,1,0],\n",
    "  [0,1,1,1,0]]\n",
    "\n",
    "#organize and label data \n",
    "train_data=[[A,1],[B,1],[C,1],[D,1],[E,1],[F,0],[G,0],[H,0],[I,0],[J,0]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TEST DATA \n",
    "K=[[1,0,0,1,0],\n",
    "  [1,0,1,0,0],\n",
    "  [1,1,0,0,0],\n",
    "  [1,0,1,0,0],\n",
    "  [1,0,0,1,0]]\n",
    "L=[[1,0,0,0,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,1,1,1,0]]\n",
    "M=[[1,1,0,1,1],\n",
    "  [1,0,1,0,1], \n",
    "  [1,0,1,0,1],\n",
    "  [1,0,0,0,1],\n",
    "  [1,0,0,0,1]]\n",
    "N=[[1,1,0,0,1],\n",
    "  [1,1,1,0,1],\n",
    "  [1,0,1,1,1],\n",
    "  [1,0,0,1,1],\n",
    "  [1,0,0,0,1]]\n",
    "O=[[0,1,1,1,0],\n",
    "  [1,0,0,0,1],\n",
    "  [1,0,0,0,1],\n",
    "  [1,0,0,0,1],\n",
    "  [0,1,1,1,0]]\n",
    "\n",
    "#organize test data \n",
    "test_data=[K,L,M,N,O]\n",
    "\n",
    "#data is currently a 2D array, has to be a vector \n",
    "\n",
    "\n",
    "#inputt is one letter \n",
    "inputt=[]\n",
    "for i in train_data: \n",
    "    j=np.array(i[0])\n",
    "    #flatten data and turn into a vector \n",
    "    inputt.append([j.flatten(),i[1]])\n",
    "\n",
    "test=[]\n",
    "for i in test_data: \n",
    "    j=np.array(i)\n",
    "    #flatten test data \n",
    "    test.append(j.flatten())\n",
    "\n",
    "#assign random values for weights that are the same size as input vector (5x5=25 weights)    \n",
    "def gen_weights(inputt): \n",
    "    weights=np.random.rand(inputt)\n",
    "    return weights\n",
    "\n",
    "#find dot product between input layer and weights to get output layer \n",
    "def calc_layer(weights,inputt):\n",
    "    for i in inputt:\n",
    "        output= np.dot(weights,inputt)\n",
    "    return output\n",
    "    \n",
    "#not needed... use to check performance    \n",
    "def calc_cost(output,labels): \n",
    "    cost=(output-labels)**2\n",
    "    return cost\n",
    "\n",
    "#claculate derivate\n",
    "    #one layer, one set of derivatives \n",
    "    #derivative of cost function \n",
    "def derivative(inputt,output,labels): \n",
    "    deriv=2*(output-labels)*inputt\n",
    "    return deriv\n",
    "    \n",
    "#caclulate gradient descent \n",
    "def gradient_descent(cost_deriv,weights):\n",
    "    #new weights \n",
    "    weights=weights-cost_deriv\n",
    "    return weights\n",
    "\n",
    "\n",
    "\n",
    "#train model\n",
    "def train_model(inputt):\n",
    "    weights=gen_weights(len(inputt[0][0]))\n",
    "    x=0\n",
    "    #ten iterations. If too small it won't learn and if its too high it'll overfit \n",
    "    while x<10:\n",
    "        for i in range(len(inputt)): \n",
    "            #print(weights)\n",
    "            #print(inputt[i])\n",
    "            output=calc_layer(weights,inputt[i][0])\n",
    "            cost=calc_cost(output,inputt[i][1])\n",
    "            cost_deriv=derivative(inputt[i][0],output,inputt[i][1])\n",
    "            #update weights \n",
    "            new_weights=gradient_descent(cost_deriv,weights)\n",
    "        x+=1\n",
    "    return weights\n",
    "\n",
    "\n",
    "def use_model(test_data,weights): \n",
    "    for item in test_data:\n",
    "        output=calc_layer(weights,item)\n",
    "        print(output)\n",
    "    return \n",
    "\n",
    "#for item in test: \n",
    "    #print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7276564473183873\n",
      "4.210390406441476\n",
      "5.164771940095489\n",
      "7.1437024543452665\n",
      "5.7969059821001245\n"
     ]
    }
   ],
   "source": [
    "#weights1=trained weights\n",
    "weights1=train_model(inputt)\n",
    "#use these trained weights to find the dot product between the input letter and the weights \n",
    "#will result in output layer \n",
    "output=use_model(test,weights1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#just to test letters \n",
    "plt.imshow(O,cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOGISTIC REGRESSION MODEL WITH GIVEN DATA \n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#CLASS A (1)\n",
    "A=[[0,0,1,0,0],\n",
    "  [0,1,0,1,0],\n",
    "  [1,0,0,0,1],\n",
    "  [1,1,1,1,1],\n",
    "  [1,0,0,0,1]]\n",
    "B=[[1,1,1,0,0],\n",
    "  [1,0,0,1,0],\n",
    "  [1,1,1,0,0],\n",
    "  [1,0,0,1,0],\n",
    "  [1,1,1,0,0]]\n",
    "C=[[0,1,1,0,0],\n",
    "  [1,0,0,1,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,0,0,1,0],\n",
    "  [0,1,1,0,0]]\n",
    "D=[[1,1,1,0,0],\n",
    "  [1,0,0,1,0],\n",
    "  [1,0,0,1,0],\n",
    "  [1,0,0,1,0],\n",
    "  [1,1,1,0,0]]\n",
    "E=[[1,1,1,1,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,1,1,1,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,1,1,1,0]]\n",
    "\n",
    "#CLASS B (0)\n",
    "F=[[1,1,1,1,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,1,1,0,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,0,0,0,0]]\n",
    "G=[[1,1,1,1,0],\n",
    "  [1,0,0,1,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,0,1,1,1],\n",
    "  [1,1,1,1,0]]\n",
    "H=[[1,0,0,0,1],\n",
    "  [1,0,0,0,1],\n",
    "  [1,1,1,1,1],\n",
    "  [1,0,0,0,1],\n",
    "  [1,0,0,0,1]]\n",
    "I=[[1,1,1,1,1],\n",
    "  [0,0,1,0,0],\n",
    "  [0,0,1,0,0],\n",
    "  [0,0,1,0,0],\n",
    "  [1,1,1,1,1]]\n",
    "J=[[1,1,1,1,0],\n",
    "  [0,1,0,0,0],\n",
    "  [0,1,0,0,0],\n",
    "  [0,1,0,1,0],\n",
    "  [0,1,1,1,0]]\n",
    "\n",
    "\n",
    "train_data=[[A,1],[B,1],[C,1],[D,1],[E,1],[F,0],[G,0],[H,0],[I,0],[J,0]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TEST DATA \n",
    "K=[[1,0,0,1,0],\n",
    "  [1,0,1,0,0],\n",
    "  [1,1,0,0,0],\n",
    "  [1,0,1,0,0],\n",
    "  [1,0,0,1,0]]\n",
    "L=[[1,0,0,0,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,1,1,1,0]]\n",
    "M=[[1,1,0,1,1],\n",
    "  [1,0,1,0,1], \n",
    "  [1,0,1,0,1],\n",
    "  [1,0,0,0,1],\n",
    "  [1,0,0,0,1]]\n",
    "N=[[1,1,0,0,1],\n",
    "  [1,1,1,0,1],\n",
    "  [1,0,1,1,1],\n",
    "  [1,0,0,1,1],\n",
    "  [1,0,0,0,1]]\n",
    "O=[[0,1,1,1,0],\n",
    "  [1,0,0,0,1],\n",
    "  [1,0,0,0,1],\n",
    "  [1,0,0,0,1],\n",
    "  [0,1,1,1,0]]\n",
    "\n",
    "\n",
    "test_data=[K,L,M,N,O]\n",
    "\n",
    "#everything is basically the same as linear regression model \n",
    "\n",
    "inputt=[]\n",
    "for i in train_data: \n",
    "    j=np.array(i[0])\n",
    "    inputt.append([j.flatten(),i[1]])\n",
    "\n",
    "test=[]\n",
    "for i in test_data: \n",
    "    j=np.array(i)\n",
    "    test.append(j.flatten())\n",
    "\n",
    "    \n",
    "def gen_weights(inputt): \n",
    "    weights=np.random.rand(inputt)\n",
    "    return weights \n",
    "    \n",
    "\n",
    "def calc_layer(weights,inputt):\n",
    "    for i in inputt:\n",
    "        output= np.dot(weights,inputt)\n",
    "    return output\n",
    "\n",
    "#will wrap sigmoid function around cost to create an S shaped curve \n",
    "#hsould create a distinction between class A and B \n",
    "def sigmoid(output):\n",
    "    #from colrization project  \n",
    "    result = 1/(1 + np.exp(-output))\n",
    "    return result\n",
    "    \n",
    "def calc_cost(output,labels): \n",
    "    cost=(output-labels)**2\n",
    "    return cost\n",
    "\n",
    "#claculate derivate\n",
    "    #one layer, one set of derivatives \n",
    "    #derivative of loss function \n",
    "def derivative(inputt,output,labels): \n",
    "    deriv=2*(output-labels)*inputt\n",
    "    return deriv\n",
    "    \n",
    "#caclulate gradient descent \n",
    "def gradient_descent(cost_deriv,weights):\n",
    "    #learning rate, this will help decrease the output values so they tend to 0 \n",
    "    weights=weights-(.0000001*cost_deriv)\n",
    "    return weights\n",
    "\n",
    "\n",
    "\n",
    "#train model\n",
    "def train_model(inputt):\n",
    "    weights=gen_weights(len(inputt[0][0]))\n",
    "    x=0\n",
    "    #10 iterations didnt really help this time \n",
    "    #while x<10:\n",
    "    for i in range(len(inputt)): \n",
    "        #print(weights)\n",
    "        #print(inputt[i])\n",
    "        output=calc_layer(weights,inputt[i][0])\n",
    "        #find original output and make it a function of sigmoid \n",
    "        output2=sigmoid(output)\n",
    "        cost=calc_cost(output2,inputt[i][1])\n",
    "        cost_deriv=derivative(inputt[i][0],output2,inputt[i][1])\n",
    "        #update weights \n",
    "        new_weights=gradient_descent(cost_deriv,weights)\n",
    "        #x+=1\n",
    "    return weights\n",
    "\n",
    "\n",
    "def use_model(test_data,weights): \n",
    "    for item in test_data:\n",
    "        output=calc_layer(weights,item)\n",
    "        output2=sigmoid(output)\n",
    "        print(output2)\n",
    "    return \n",
    "\n",
    "#for item in test: \n",
    "    #print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9739682534315217\n",
      "0.9270640451957488\n",
      "0.9971042103248876\n",
      "0.9987291633808327\n",
      "0.9688727814938872\n"
     ]
    }
   ],
   "source": [
    "weights1=train_model(inputt)\n",
    "output=use_model(test,weights1)\n",
    "#results are better than linear regression, but if i rounded them they would all be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "#LOGISTIC REGRESSION WITH AUGMENTED DATA \n",
    "\n",
    "from random import shuffle \n",
    "\n",
    "A=[[0,0,1,0,0],\n",
    "  [0,1,0,1,0],\n",
    "  [1,0,0,0,1],\n",
    "  [1,1,1,1,1],\n",
    "  [1,0,0,0,1]]\n",
    "B=[[1,1,1,0,0],\n",
    "  [1,0,0,1,0],\n",
    "  [1,1,1,0,0],\n",
    "  [1,0,0,1,0],\n",
    "  [1,1,1,0,0]]\n",
    "C=[[0,1,1,0,0],\n",
    "  [1,0,0,1,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,0,0,1,0],\n",
    "  [0,1,1,0,0]]\n",
    "D=[[1,1,1,0,0],\n",
    "  [1,0,0,1,0],\n",
    "  [1,0,0,1,0],\n",
    "  [1,0,0,1,0],\n",
    "  [1,1,1,0,0]]\n",
    "E=[[1,1,1,1,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,1,1,1,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,1,1,1,0]]\n",
    "\n",
    "\n",
    "\n",
    "#CLASS B (0)\n",
    "F=[[1,1,1,1,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,1,1,0,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,0,0,0,0]]\n",
    "G=[[1,1,1,1,0],\n",
    "  [1,0,0,1,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,0,1,1,1],\n",
    "  [1,1,1,1,0]]\n",
    "H=[[1,0,0,0,1],\n",
    "  [1,0,0,0,1],\n",
    "  [1,1,1,1,1],\n",
    "  [1,0,0,0,1],\n",
    "  [1,0,0,0,1]]\n",
    "I=[[1,1,1,1,1],\n",
    "  [0,0,1,0,0],\n",
    "  [0,0,1,0,0],\n",
    "  [0,0,1,0,0],\n",
    "  [1,1,1,1,1]]\n",
    "J=[[1,1,1,1,0],\n",
    "  [0,1,0,0,0],\n",
    "  [0,1,0,0,0],\n",
    "  [0,1,0,1,0],\n",
    "  [0,1,1,1,0]]\n",
    "\n",
    "\n",
    "train_data=[[A,1],[B,1],[C,1],[D,1],[E,1],[F,0],[G,0],[H,0],[I,0],[J,0]]\n",
    "#rotate all letters so we have four different variations of each letter\n",
    "#results in 40 data points \n",
    "for item in range(0,10): \n",
    "    train_data.append([np.rot90(train_data[item][0],1),train_data[item][1]])\n",
    "    train_data.append([np.rot90(train_data[item][0],2),train_data[item][1]])\n",
    "    train_data.append([np.rot90(train_data[item][0],3),train_data[item][1]])\n",
    "print(len(train_data))\n",
    "\n",
    "#shuffle data... this helped the results a little bit. Thought it might be overfitting if it was reading \n",
    "#the letters in the same order \n",
    "shuffle(train_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test pictures and see how they look \n",
    "#for item in train_data: \n",
    "    #plt.imshow(item[0], cmap=plt.cm.binary)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data remains the same \n",
    "#TEST DATA \n",
    "K=[[1,0,0,1,0],\n",
    "  [1,0,1,0,0],\n",
    "  [1,1,0,0,0],\n",
    "  [1,0,1,0,0],\n",
    "  [1,0,0,1,0]]\n",
    "L=[[1,0,0,0,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,0,0,0,0],\n",
    "  [1,1,1,1,0]]\n",
    "M=[[1,1,0,1,1],\n",
    "  [1,0,1,0,1], \n",
    "  [1,0,1,0,1],\n",
    "  [1,0,0,0,1],\n",
    "  [1,0,0,0,1]]\n",
    "N=[[1,1,0,0,1],\n",
    "  [1,1,1,0,1],\n",
    "  [1,0,1,1,1],\n",
    "  [1,0,0,1,1],\n",
    "  [1,0,0,0,1]]\n",
    "O=[[0,1,1,1,0],\n",
    "  [1,0,0,0,1],\n",
    "  [1,0,0,0,1],\n",
    "  [1,0,0,0,1],\n",
    "  [0,1,1,1,0]]\n",
    "\n",
    "\n",
    "test_data=[K,L,M,N,O]\n",
    "\n",
    "\n",
    "\n",
    "inputt=[]\n",
    "for i in train_data: \n",
    "    j=np.array(i[0])\n",
    "    inputt.append([j.flatten(),i[1]])\n",
    "\n",
    "test=[]\n",
    "for i in test_data: \n",
    "    j=np.array(i)\n",
    "    test.append(j.flatten())\n",
    "\n",
    "    \n",
    "def gen_weights(inputt): \n",
    "    #changed the weights...the initial weight mightve been too high \n",
    "    weights=np.random.rand(inputt)/10\n",
    "    return weights \n",
    "    \n",
    "\n",
    "def calc_layer(weights,inputt):\n",
    "    for i in inputt:\n",
    "        output= np.dot(weights,inputt)\n",
    "    return output\n",
    "\n",
    "def sigmoid(output):\n",
    "    result = 1/(1 + np.exp(-output))\n",
    "    return result\n",
    "    \n",
    "def calc_cost(output,labels): \n",
    "    cost=(output-labels)**2\n",
    "    return cost\n",
    "\n",
    "#claculate derivate\n",
    "    #one layer, one set of derivatives \n",
    "    #derivative of loss function \n",
    "def derivative(inputt,output,labels): \n",
    "    deriv=2*(output-labels)*inputt\n",
    "    return deriv\n",
    "    \n",
    "#caclulate gradient descent \n",
    "def gradient_descent(cost_deriv,weights):\n",
    "    #expermineted with the learning rate \n",
    "    weights=weights-200*cost_deriv\n",
    "    return weights\n",
    "\n",
    "\n",
    "\n",
    "#train model\n",
    "def train_model(inputt):\n",
    "    weights=gen_weights(len(inputt[0][0]))\n",
    "    x=0\n",
    "    #while x<10:\n",
    "    for i in range(len(inputt)): \n",
    "        #print(weights)\n",
    "        #print(inputt[i])\n",
    "        output=calc_layer(weights,inputt[i][0])\n",
    "        output2=sigmoid(output)\n",
    "        #print('output,lab',output2,inputt[i][1])\n",
    "        cost=calc_cost(output2,inputt[i][1])\n",
    "       # print('cost',cost)\n",
    "        cost_deriv=derivative(inputt[i][0],output2,inputt[i][1])\n",
    "        #update weights \n",
    "        new_weights=gradient_descent(cost_deriv,weights)\n",
    "        #x+=1\n",
    "    return weights\n",
    "\n",
    "\n",
    "def use_model(test_data,weights): \n",
    "    for item in test_data:\n",
    "        output=calc_layer(weights,item)\n",
    "        output2=sigmoid(output)\n",
    "        print(output2)\n",
    "    return \n",
    "\n",
    "#for item in test: \n",
    "    #print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6292650801924171\n",
      "0.6237243159999414\n",
      "0.6597691358923263\n",
      "0.6989551344045165\n",
      "0.6532740126008849\n"
     ]
    }
   ],
   "source": [
    "weights1=train_model(inputt)\n",
    "output=use_model(test,weights1)\n",
    "#results are better, but there are some patterns that i can uncover "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
